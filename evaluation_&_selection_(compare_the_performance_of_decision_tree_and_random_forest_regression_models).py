# -*- coding: utf-8 -*-
"""Evaluation & Selection (Compare the performance of Decision Tree and Random Forest Regression models).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LNc1V9DmGuZ55djcS8JyzlQJhdOImE2J

# **3. Evaluation & Selection & Recap**

### **Problem Statement**  

Compare the performance of Decision Tree and Random Forest Regression models in predicting students' **GPA (Grade Point Average)** based on study habits, parental involvement, extracurricular activities, and other demographic factors. Identify which model provides better accuracy and generalization for educational performance prediction.

---

### **Dataset Overview**  
**DataSetLink**: https://www.kaggle.com/datasets/rabieelkharoua/students-performance-dataset

#### **Attributes**:  
1. **Student Information**:  
   - `StudentID`: Unique identifier for each student.  

2. **Demographic Details**:  
   - `Age`: Age of students (15–18 years).  
   - `Gender`: 0 = Male, 1 = Female.  
   - `Ethnicity`:  
     - 0: Caucasian  
     - 1: African American  
     - 2: Asian  
     - 3: Other  
   - `ParentalEducation`:  
     - 0: None  
     - 1: High School  
     - 2: Some College  
     - 3: Bachelor's  
     - 4: Higher  

3. **Study Habits**:  
   - `StudyTimeWeekly`: Weekly study time (hours, 0–20).  
   - `Absences`: Number of absences (0–30).  
   - `Tutoring`: 0 = No, 1 = Yes.  

4. **Parental Involvement**:  
   - `ParentalSupport`:  
     - 0: None  
     - 1: Low  
     - 2: Moderate  
     - 3: High  
     - 4: Very High  

5. **Extracurricular Activities**:  
   - `Extracurricular`: 0 = No, 1 = Yes.  
   - `Sports`: 0 = No, 1 = Yes.  
   - `Music`: 0 = No, 1 = Yes.  
   - `Volunteering`: 0 = No, 1 = Yes.  

6. **Academic Performance**:  
   - `GPA`: Grade Point Average (2.0–4.0).  

#### **Target Variable**:  
- **GradeClass**: Classification of grades based on GPA:  
  - 0: 'A' (GPA ≥ 3.5)  
  - 1: 'B' (3.0 ≤ GPA < 3.5)  
  - 2: 'C' (2.5 ≤ GPA < 3.0)  
  - 3: 'D' (2.0 ≤ GPA < 2.5)  
  - 4: 'F' (GPA < 2.0)  

---

### **Instructions**  

1. **Data Preparation**  
   - Load the dataset and explore the features.  
   - Drop irrelevant columns such as `StudentID`.  
   - Ensure there are no missing values; handle them if present.  
   - Normalize or scale continuous features (`StudyTimeWeekly`, `Absences`, etc.) if necessary.  

2. **Model Training**  
   - Train a **Decision Tree Regressor** and a **Random Forest Regressor** on the dataset using `GPA` as the target variable.  
   - Use 80% of the data for training and 20% for testing.  

3. **Evaluation**  
   - Evaluate both models using:  
     - **Mean Squared Error (MSE)**  
     - **R² Score**  
     - **Mean Absolute Error (MAE)**  
   - Compare the results for both models.  

4. **Visualization**  
   - Plot the **predicted vs. actual values** for both models.  
   - Visualize the feature importance for the Random Forest model.  

5. **Report Writing**  
   - Write a brief report summarizing:  
     - Model performance.  
     - Key features influencing the predictions.  
     - Which model performed better and why.  

---
"""

import pandas as pd

data = pd.read_csv("Student_performance_data _.csv")

print(data.head())

data = data.drop(columns=["StudentID"])

print("Missing values per column:\n", data.isnull().sum())

data = data.fillna(data.mean())

print(data.describe())

from sklearn.preprocessing import MinMaxScaler

continuous_features = ["Age", "StudyTimeWeekly", "Absences", "ParentalSupport"]

scaler = MinMaxScaler()
data[continuous_features] = scaler.fit_transform(data[continuous_features])

print(data[continuous_features].head())

from sklearn.model_selection import train_test_split

X = data.drop(columns=["GPA", "GradeClass"])
y = data["GPA"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape[0]}, Testing set size: {X_test.shape[0]}")

from sklearn.tree import DecisionTreeRegressor

dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

dt_predictions = dt_model.predict(X_test)

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(random_state=42, n_estimators=100)
rf_model.fit(X_train, y_train)

rf_predictions = rf_model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Decision Tree Evaluation
dt_mse = mean_squared_error(y_test, dt_predictions)
dt_r2 = r2_score(y_test, dt_predictions)
dt_mae = mean_absolute_error(y_test, dt_predictions)

print("Decision Tree Performance:")
print(f"Mean Squared Error: {dt_mse}")
print(f"R² Score: {dt_r2}")
print(f"Mean Absolute Error: {dt_mae}")

# Random Forest Evaluation
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)
rf_mae = mean_absolute_error(y_test, rf_predictions)

print("\nRandom Forest Performance:")
print(f"Mean Squared Error: {rf_mse}")
print(f"R² Score: {rf_r2}")
print(f"Mean Absolute Error: {rf_mae}")

import matplotlib.pyplot as plt

# Decision Tree: Predicted vs Actual
plt.figure(figsize=(12, 6))
plt.scatter(y_test, dt_predictions, color="blue", alpha=0.5, label="Decision Tree Predictions")
plt.scatter(y_test, rf_predictions, color="green", alpha=0.5, label="Random Forest Predictions")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label="Perfect Prediction")
plt.xlabel("Actual GPA")
plt.ylabel("Predicted GPA")
plt.title("Predicted vs Actual GPA")
plt.legend()
plt.show()

# Extracting feature importance
feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": rf_model.feature_importances_
}).sort_values(by="Importance", ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance["Feature"], feature_importance["Importance"], color="skyblue")
plt.xlabel("Feature Importance")
plt.title("Random Forest Feature Importance")
plt.gca().invert_yaxis()
plt.show()

